# Development can be found at 
# A pseudocode for a graph neural network is included at the bottom for reference.

# Graph net boosted prolongation operator for algebraic multigrid 
# (Graph net may be generalised to other ML techniques)
# This pseudocode trains a graph neural network to select the interpolation weights in the 
#	prolongation operator for transporting errors between grids in the algebraic multigrid (AMG) 
#	algorithm.
# AMG is an iterative algorithm for solving large and parse linear systems that arise from, for example,
#   discretisation of partial differential equations (PDEs). In each iteration, high frequency errors 
#	are eliminated by a classic relaxation scheme (commonly Gauss-Seidel), whereas low-frequency errors 
#	(i.e. smooth errors) are transferred onto a coarser grid where the smooth errors can be more 
#	efficiently removed by further relation sweeps. Multiple levels of such grids of increasing coarseness 
#	can be used for reducing the error. The interpolation operators transport errors between adjacent grid 
#	levels.
# The coefficient matrix of a coarse grid, A_coarse, is commonly generated by the standard Galerkin 
#	projection: A_coarse = R*A*P = P_transpose*A*P, where the restriction matrix R (fine->coarse grid) 
#	is just the transpose of the prolongation matrix P (coarse->fine grid). P and R are the interpolation 
#	operators.
# In particular, we use a neural network to help build the prolongation operator P that interpolates 
#	errors from a coarse grid to a fine one. This is done with the information of the coefficient matrix A 
#	of the finest grid, and the standard prolongation operator P_baseline generated by a traditional 
#	method. Although the sparsity pattern of P is enforced to be the same as P_baseline, the non-zero 
#	values (interpolation weights) in P are optimised through the neural network. Let matrix M describe 
#	the error propagation from one iteration to another by e_(i+1) = M * e_i, then the goal of an optimal 
#	P is to minimise M. In AMG, M = S_s1 * C * S_s2. S_s1 and S_s2 are the relaxation operators applied on 
#	the finest grid, before and after the cycle through the coarse grids, where s1 and s2 refer to the 
#	number of relaxation sweeps and are both typically 1 or 2. C represents the error reduction achieved 
#	through the coarse grids. C = I − P*[P_transpose*A*P]^(−1)*P_transpose*A. Given S_s1 and S_s2, which 
#	themselves typically do not have much room to be optimised, the choice of P determines the efficiency 
#	of error reduction per iteration. We use the Frobenius norm of M as the loss function of P. 
# The objective of the neural network can be summarised as follows: given A and P_baseline, train P to 
#	minimise M. A is a matrix of size mxm, where m is the dimension of the unknowns in the linear system, 
#	i.e. the number of nodes in the finest grid. P_baseline is a matrix of size mxn, where n is the number
#	of nodes in the coarser grid. n<m and typically n~m/2. P takes the same dimension and sparsity pattern
#	of P_baseline. The problem can be naturally represented by a graph neural network where A is the 
#	adjacency matrix. The elements of A are an edge feature, representing the level of 
#	influence/dependency between two vertices. In our case, these elements represent the "closeness" of
#	nodes, a concept that is natural in a geometrical multigrid problem but needs to be defined 
#	analogously in an algebraic multigrid problem. The GN then needs to output a graph with updated edge
#	features. The output graph has the same dimensions as the input graph. But with a few simple steps, 
#	the new edge features can be used to form the non-zero elements of P.
# AMG methods are originally developed for As that are M-matrices, which are matrices that are symmetric 
#	and positive definite with positive entries on the diagonal and nonpositive off-diagonal entries. This 
#	assumption is not necessary for AMG to work, but standard AMG is less likely to be effective if As are
#	far from M-matrices. In Luz's work, As are chosen to be sparse symmetric positive definite or 
#	semi-definite matrices.


# === main ===
# Pre:
# Post:
def main():
	# these variables will be moved to an input file later
	n_samples = 1000
	matrix_type = "BCLaplacian"
	splitting_method = "CLJP"

	# generate training As
	training_As = generate_matrices(n_samples, n_rows, n_cols, matrix_type)

	for i in range(n_samples):
		A = training_As[i]

		# get Ruge-Stüben solver
		solver = pyamg.ruge_stuben_solver(A, max-levels=2, keep=True, CF=splitting_method)

		# get coarse-fine splitting
		splitting = solver.levels[0].splitting
		# get baseline P generated by traditional solver
		P_baseline = direct_interpolation(A, C, splitting)

		# apply graph net to transform As
		P = apply_GN(A, n_GNlayers)
		set the diagonal of P to 1
		remove all columns of P associated with fine nodes
		# note P is mxn for A of mxm and A_coarse of nxn
		apply the sparsity pattern of P_base to P
		rescale rows of P to have the same row sums as P_base

		# measure the asymptotic convergence factors from both classical method and our graphNet Ps
		asymConvFactor(P_baseline)
		asymConvFactor(P)

	visualise differences between P_baseline and P


# === generate_matrices ===
Pre: given inputs of 
	- n_samples, int, number of samples
	- n_rows, int
	- n_cols, int
	- matrix_type, int, a flag for the type of matrices to be generated. We can use this flag later to 
		switch among a few choices of different types. Default for now is block-circulant Laplacian 
		matrices
Post: output a set of n_samples matrices, based on the specified type
def generate_matrices(n_samples, n_rows, n_cols, matrix_type):
	if n_samples < 1:
		# throw error
	if matrix_type != "BCLaplacian":
		# output "this type is not implemented yet"
	# generate matrices
	# return matrices


# === apply_GN ===
Pre: given inputs of
	- adjacency matrix A
	- n_GNlayers, int, number of GN layers i.e. maximum distance of vertex input in number of hops away
Post: an output matrix of the same dimensions as A
def apply_GN(A, n_GNlayers):
	apply As to graph
	expand the number of edge and vertex features to n_GNfeatures by going through an mlp encoder
	for i=n_GNlayers
		apply graphNet
	mlp decoder from n_GNfeatures to 1 node feature and 1 edge feature
	return output matrix


# === graph net ===
# Input:
#	- edgeFeatures # set of edge features edgeFeatures, size n_edge x n_feature
#	- vertexFeatures # set of vertex features, size n_node x n_feature
#	- u # global features, in our case there may not be any such features
#	- sendingVertices # set of sending vertex indices for each edge, size n_edge
#	- receivingVertices # set of receiving vertex indices for each edge, size n_edge
#	- updateEdge, updateNode, updateGlobal # update functions for edges, vertices, global features
#	- aggrEdge, aggrNode, aggrGlobal # aggregation functions for edges, vertices, global features
# Output:
#	- edgeFeatures_new # updated edge features
#	- vertexFeatures_new # updated node features
#	- u_new # updated global features, if global features are used
graphNet
    for k = 0 to n_edge-1 # For each edge
		# Update edge features 
        edgeFeatures_new[k] = updateEdge(edgeFeatures[k], receivingVertices[k], sendingVertices[k], u) 
    end for 
    for i = 0 to n_node-1 # For each vertex 
		# get edge indices with vertex i as receiving vertex 
        edgeFeatures[i] = indices of elements of receivingVertices where vertex index value = i
		# Aggregate all edge features for edges with indices in edgeFeatures[i] 
        edgeFeatures_new[i] = aggrEdge({edgeFeatures_new[j] where j is in edgeFeatures[i]}) 
		# Update vertex features 
        vertexFeatures_new[I] = updateNode(edgeFeatures_new[i], vertexFeatures[I], u) 
    end for
    aggrEdge(edgeFeatures_new) # Aggregate all edge features 
    aggrNode(vertexFeatures_new) # Aggregate all vertex features 
    u_new = aggrGlobal(u) # Update global features 
    return (edgeFeatures_new, vertexFeatures_new, u_new) 
end function